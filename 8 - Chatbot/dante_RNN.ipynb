{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dante_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/8%20-%20Chatbot/dante_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "q3cb6BFDHJ44",
        "colab_type": "code",
        "outputId": "45837586-9861-44b1-defc-d7959d3adcc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.liberliber.it/mediateca/libri/a/alighieri/la_divina_commedia/txt/la_divin.zip"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-20 16:52:32--  https://www.liberliber.it/mediateca/libri/a/alighieri/la_divina_commedia/txt/la_divin.zip\n",
            "Resolving www.liberliber.it (www.liberliber.it)... 93.186.244.67\n",
            "Connecting to www.liberliber.it (www.liberliber.it)|93.186.244.67|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 232691 (227K) [application/zip]\n",
            "Saving to: ‘la_divin.zip.1’\n",
            "\n",
            "la_divin.zip.1      100%[===================>] 227.24K   466KB/s    in 0.5s    \n",
            "\n",
            "2019-04-20 16:52:33 (466 KB/s) - ‘la_divin.zip.1’ saved [232691/232691]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_kJMMCZmIgvK",
        "colab_type": "code",
        "outputId": "bb929ee2-c34a-4e11-a786-8c1a86837377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip la_divin.zip"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  la_divin.zip\n",
            "replace la_divin.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r6Jb2lDdJ4rJ",
        "colab_type": "code",
        "outputId": "9475ff6b-4d99-4a58-bd4f-0a07e59dc06a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "cell_type": "code",
      "source": [
        "with open(\"la_divin.txt\", encoding=\"latin-1\") as divine_file:\n",
        "  divine_txt = divine_file.read()\n",
        "  \n",
        "print(divine_txt[:100])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dante Alighieri\n",
            "La Divina Commedia\n",
            "\n",
            "Questo e-book è stato realizzato anche grazie al sostegno di:\n",
            "E-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l8fKWhRlKLPR",
        "colab_type": "code",
        "outputId": "0fe3a3e1-dca7-4e00-f858-da693b8ad5cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "start = divine_txt.find(\"Nel mezzo del cammin di nostra vita\")\n",
        "end = divine_txt.find(\"l'amor che move il sole e l'altre stelle.\")\n",
        "\n",
        "divine_txt = divine_txt[start:end]\n",
        "\n",
        "print(divine_txt[:100])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nel mezzo del cammin di nostra vita\n",
            "mi ritrovai per una selva oscura,\n",
            "ché la diritta via era smarrit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R_WfHALrSGQG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "divine_txt = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", divine_txt)\n",
        "divine_txt = re.sub(\"[A-Z][A-Z]+\",\"\",divine_txt)\n",
        "\n",
        "divine_txt = divine_txt.replace(\"di Dante Alighieri\",\"\")\n",
        "\n",
        "divine_txt = re.sub(r'[^\\w\\s]','',divine_txt)\n",
        "divine_txt = divine_txt.replace(\"\\n\",\" \")\n",
        "divine_txt = divine_txt.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1p0leKS7ed-F",
        "colab_type": "code",
        "outputId": "73464367-9f0e-4afd-ab60-16ebda93ffff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "cell_type": "code",
      "source": [
        "!python -m spacy download it_core_news_sm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: it_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-2.0.0/it_core_news_sm-2.0.0.tar.gz#egg=it_core_news_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/it_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/it_core_news_sm\n",
            "\n",
            "    You can now load the model via spacy.load('it_core_news_sm')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Uw_ynE1men3a",
        "colab_type": "code",
        "outputId": "ce39c33f-48df-4789-c5ab-a193044c1826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"it_core_news_sm\")\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "  tokens = nlp(text)\n",
        "  tokens_filtered = [token.text for token in tokens]\n",
        "  return tokens_filtered\n",
        "\n",
        "tokens = preprocess(divine_txt)\n",
        "tokens[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['nel',\n",
              " 'mezzo',\n",
              " 'del',\n",
              " 'cammin',\n",
              " 'di',\n",
              " 'nostra',\n",
              " 'vita',\n",
              " 'mi',\n",
              " 'ritrovai',\n",
              " 'per']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "6oXESRj8dUx3",
        "colab_type": "code",
        "outputId": "53a884cf-bf21-4850-f5f9-d7a034f2af80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "maxlen = 10\n",
        "\n",
        "divine_sents = []\n",
        "\n",
        "for i in range(maxlen, len(tokens)):\n",
        "  divine_sents.append(tokens[i-maxlen:i])\n",
        "  \n",
        "print(divine_sents[0])\n",
        "print(divine_sents[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['nel', 'mezzo', 'del', 'cammin', 'di', 'nostra', 'vita', 'mi', 'ritrovai', 'per']\n",
            "['mezzo', 'del', 'cammin', 'di', 'nostra', 'vita', 'mi', 'ritrovai', 'per', 'una']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZLBDHWQZWpm8",
        "colab_type": "code",
        "outputId": "f474a61c-6a65-4226-91a5-31525e56a0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(divine_sents)\n",
        "divine_sents = tokenizer.texts_to_sequences(divine_sents)\n",
        "\n",
        "divine_sents[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[38, 224, 24, 603, 4, 186, 153, 15, 13574, 7]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "24kyU6HsiTf5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "divine_sents = np.array(divine_sents)\n",
        "\n",
        "X = divine_sents[:,:-1]\n",
        "Y = divine_sents[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UteQnvyqcXbl",
        "colab_type": "code",
        "outputId": "3a941a5a-3f8b-424a-f232-bf7e8de94f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "vocab_size = len(tokenizer.word_counts)\n",
        "\n",
        "Y = to_categorical(Y, num_classes=vocab_size+1)\n",
        "Y.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97393, 13575)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "zunahaBbkbdc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d7bef896-a78e-4897-d57b-b80cbb3a9edf"
      },
      "cell_type": "code",
      "source": [
        "from keras import Model, Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size+1, maxlen-1, input_length=maxlen-1))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(50, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size+1, activation=\"softmax\"))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Y29LJ9Dk596",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TEjQFmPklCQ4",
        "colab_type": "code",
        "outputId": "e56197b8-11f2-4b40-dde3-f41fcfa48572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3534
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(X, Y, batch_size=128, epochs=100)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            "97393/97393 [==============================] - 41s 420us/step - loss: 7.4553 - acc: 0.0392\n",
            "Epoch 2/100\n",
            "97393/97393 [==============================] - 37s 384us/step - loss: 7.0885 - acc: 0.0407\n",
            "Epoch 3/100\n",
            "97393/97393 [==============================] - 37s 376us/step - loss: 6.9740 - acc: 0.0458\n",
            "Epoch 4/100\n",
            "97393/97393 [==============================] - 37s 385us/step - loss: 6.8586 - acc: 0.0538\n",
            "Epoch 5/100\n",
            "97393/97393 [==============================] - 37s 379us/step - loss: 6.7773 - acc: 0.0569\n",
            "Epoch 6/100\n",
            "97393/97393 [==============================] - 37s 383us/step - loss: 6.7014 - acc: 0.0587\n",
            "Epoch 7/100\n",
            "97393/97393 [==============================] - 36s 374us/step - loss: 6.6215 - acc: 0.0606\n",
            "Epoch 8/100\n",
            "97393/97393 [==============================] - 37s 384us/step - loss: 6.5462 - acc: 0.0625\n",
            "Epoch 9/100\n",
            "97393/97393 [==============================] - 37s 379us/step - loss: 6.4741 - acc: 0.0641\n",
            "Epoch 10/100\n",
            "97393/97393 [==============================] - 38s 390us/step - loss: 6.4011 - acc: 0.0659\n",
            "Epoch 11/100\n",
            "97393/97393 [==============================] - 37s 376us/step - loss: 6.3254 - acc: 0.0687\n",
            "Epoch 12/100\n",
            "97393/97393 [==============================] - 36s 371us/step - loss: 6.2467 - acc: 0.0730\n",
            "Epoch 13/100\n",
            "97393/97393 [==============================] - 38s 388us/step - loss: 6.1637 - acc: 0.0754\n",
            "Epoch 14/100\n",
            "97393/97393 [==============================] - 36s 370us/step - loss: 6.0751 - acc: 0.0782\n",
            "Epoch 15/100\n",
            "97393/97393 [==============================] - 37s 380us/step - loss: 5.9859 - acc: 0.0807\n",
            "Epoch 16/100\n",
            "97393/97393 [==============================] - 37s 383us/step - loss: 5.8983 - acc: 0.0833\n",
            "Epoch 17/100\n",
            "97393/97393 [==============================] - 38s 386us/step - loss: 5.8167 - acc: 0.0861\n",
            "Epoch 18/100\n",
            "97393/97393 [==============================] - 38s 390us/step - loss: 5.7376 - acc: 0.0874\n",
            "Epoch 19/100\n",
            "97393/97393 [==============================] - 38s 385us/step - loss: 5.6639 - acc: 0.0895\n",
            "Epoch 20/100\n",
            "97393/97393 [==============================] - 36s 374us/step - loss: 5.5917 - acc: 0.0913\n",
            "Epoch 21/100\n",
            "97393/97393 [==============================] - 37s 384us/step - loss: 5.5217 - acc: 0.0930\n",
            "Epoch 22/100\n",
            "97393/97393 [==============================] - 36s 373us/step - loss: 5.4544 - acc: 0.0954\n",
            "Epoch 23/100\n",
            "97393/97393 [==============================] - 37s 378us/step - loss: 5.3863 - acc: 0.0966\n",
            "Epoch 24/100\n",
            "97393/97393 [==============================] - 37s 376us/step - loss: 5.3247 - acc: 0.0985\n",
            "Epoch 25/100\n",
            "97393/97393 [==============================] - 36s 372us/step - loss: 5.2619 - acc: 0.0999\n",
            "Epoch 26/100\n",
            "97393/97393 [==============================] - 39s 396us/step - loss: 5.2057 - acc: 0.1019\n",
            "Epoch 27/100\n",
            "97393/97393 [==============================] - 36s 371us/step - loss: 5.1520 - acc: 0.1036\n",
            "Epoch 28/100\n",
            "97393/97393 [==============================] - 37s 382us/step - loss: 5.0978 - acc: 0.1071\n",
            "Epoch 29/100\n",
            "97393/97393 [==============================] - 36s 371us/step - loss: 5.0487 - acc: 0.1093\n",
            "Epoch 30/100\n",
            "97393/97393 [==============================] - 38s 388us/step - loss: 5.0040 - acc: 0.1120\n",
            "Epoch 31/100\n",
            "97393/97393 [==============================] - 36s 371us/step - loss: 4.9619 - acc: 0.1149\n",
            "Epoch 32/100\n",
            "97393/97393 [==============================] - 37s 379us/step - loss: 4.9210 - acc: 0.1179\n",
            "Epoch 33/100\n",
            "97393/97393 [==============================] - 36s 372us/step - loss: 4.8850 - acc: 0.1204\n",
            "Epoch 34/100\n",
            "97393/97393 [==============================] - 37s 379us/step - loss: 4.8445 - acc: 0.1247\n",
            "Epoch 35/100\n",
            "97393/97393 [==============================] - 37s 385us/step - loss: 4.8136 - acc: 0.1264\n",
            "Epoch 36/100\n",
            "97393/97393 [==============================] - 36s 374us/step - loss: 4.7806 - acc: 0.1295\n",
            "Epoch 37/100\n",
            "97393/97393 [==============================] - 37s 377us/step - loss: 4.7480 - acc: 0.1318\n",
            "Epoch 38/100\n",
            "97393/97393 [==============================] - 37s 375us/step - loss: 4.7135 - acc: 0.1358\n",
            "Epoch 39/100\n",
            "97393/97393 [==============================] - 37s 380us/step - loss: 4.6873 - acc: 0.1384\n",
            "Epoch 40/100\n",
            "97393/97393 [==============================] - 36s 369us/step - loss: 4.6546 - acc: 0.1412\n",
            "Epoch 41/100\n",
            "97393/97393 [==============================] - 37s 379us/step - loss: 4.6272 - acc: 0.1443\n",
            "Epoch 42/100\n",
            "97393/97393 [==============================] - 36s 367us/step - loss: 4.5978 - acc: 0.1477\n",
            "Epoch 43/100\n",
            "97393/97393 [==============================] - 37s 382us/step - loss: 4.5700 - acc: 0.1490\n",
            "Epoch 44/100\n",
            "97393/97393 [==============================] - 36s 367us/step - loss: 4.5398 - acc: 0.1531\n",
            "Epoch 45/100\n",
            "97393/97393 [==============================] - 36s 374us/step - loss: 4.5126 - acc: 0.1541\n",
            "Epoch 46/100\n",
            "97393/97393 [==============================] - 35s 363us/step - loss: 4.4878 - acc: 0.1574\n",
            "Epoch 47/100\n",
            "97393/97393 [==============================] - 35s 363us/step - loss: 4.4592 - acc: 0.1602\n",
            "Epoch 48/100\n",
            "97393/97393 [==============================] - 35s 364us/step - loss: 4.4306 - acc: 0.1631\n",
            "Epoch 49/100\n",
            "97393/97393 [==============================] - 35s 356us/step - loss: 4.4005 - acc: 0.1674\n",
            "Epoch 50/100\n",
            "97393/97393 [==============================] - 36s 365us/step - loss: 4.3749 - acc: 0.1689\n",
            "Epoch 51/100\n",
            "97393/97393 [==============================] - 35s 355us/step - loss: 4.3534 - acc: 0.1713\n",
            "Epoch 52/100\n",
            "97393/97393 [==============================] - 37s 378us/step - loss: 4.3250 - acc: 0.1745\n",
            "Epoch 53/100\n",
            "97393/97393 [==============================] - 35s 358us/step - loss: 4.3036 - acc: 0.1783\n",
            "Epoch 54/100\n",
            "97393/97393 [==============================] - 36s 365us/step - loss: 4.2819 - acc: 0.1795\n",
            "Epoch 55/100\n",
            "97393/97393 [==============================] - 36s 368us/step - loss: 4.2591 - acc: 0.1820\n",
            "Epoch 56/100\n",
            "97393/97393 [==============================] - 35s 360us/step - loss: 4.2351 - acc: 0.1858\n",
            "Epoch 57/100\n",
            "97393/97393 [==============================] - 36s 368us/step - loss: 4.2100 - acc: 0.1892\n",
            "Epoch 58/100\n",
            "97393/97393 [==============================] - 35s 360us/step - loss: 4.1893 - acc: 0.1912\n",
            "Epoch 59/100\n",
            "97393/97393 [==============================] - 36s 370us/step - loss: 4.1707 - acc: 0.1933\n",
            "Epoch 60/100\n",
            "97393/97393 [==============================] - 35s 359us/step - loss: 4.1530 - acc: 0.1956\n",
            "Epoch 61/100\n",
            "97393/97393 [==============================] - 37s 380us/step - loss: 4.1292 - acc: 0.1984\n",
            "Epoch 62/100\n",
            "97393/97393 [==============================] - 35s 361us/step - loss: 4.1052 - acc: 0.2028\n",
            "Epoch 63/100\n",
            "97393/97393 [==============================] - 36s 366us/step - loss: 4.0913 - acc: 0.2037\n",
            "Epoch 64/100\n",
            "97393/97393 [==============================] - 36s 367us/step - loss: 4.0729 - acc: 0.2060\n",
            "Epoch 65/100\n",
            "97393/97393 [==============================] - 35s 363us/step - loss: 4.0523 - acc: 0.2085\n",
            "Epoch 66/100\n",
            "97393/97393 [==============================] - 36s 369us/step - loss: 4.0315 - acc: 0.2103\n",
            "Epoch 67/100\n",
            "97393/97393 [==============================] - 35s 362us/step - loss: 4.0091 - acc: 0.2138\n",
            "Epoch 68/100\n",
            "97393/97393 [==============================] - 36s 366us/step - loss: 3.9875 - acc: 0.2169\n",
            "Epoch 69/100\n",
            "97393/97393 [==============================] - 36s 370us/step - loss: 3.9754 - acc: 0.2192\n",
            "Epoch 70/100\n",
            "97393/97393 [==============================] - 38s 388us/step - loss: 3.9581 - acc: 0.2210\n",
            "Epoch 71/100\n",
            "97393/97393 [==============================] - 36s 372us/step - loss: 3.9427 - acc: 0.2229\n",
            "Epoch 72/100\n",
            "97393/97393 [==============================] - 38s 391us/step - loss: 3.9254 - acc: 0.2263\n",
            "Epoch 73/100\n",
            "97393/97393 [==============================] - 36s 369us/step - loss: 3.9063 - acc: 0.2283\n",
            "Epoch 74/100\n",
            "97393/97393 [==============================] - 35s 362us/step - loss: 3.8909 - acc: 0.2291\n",
            "Epoch 75/100\n",
            "97393/97393 [==============================] - 36s 365us/step - loss: 3.8757 - acc: 0.2325\n",
            "Epoch 76/100\n",
            "97393/97393 [==============================] - 35s 358us/step - loss: 3.8585 - acc: 0.2355\n",
            "Epoch 77/100\n",
            "97393/97393 [==============================] - 35s 364us/step - loss: 3.8401 - acc: 0.2372\n",
            "Epoch 78/100\n",
            "97393/97393 [==============================] - 36s 369us/step - loss: 3.8311 - acc: 0.2383\n",
            "Epoch 79/100\n",
            "97393/97393 [==============================] - 36s 367us/step - loss: 3.8176 - acc: 0.2401\n",
            "Epoch 80/100\n",
            "97393/97393 [==============================] - 35s 362us/step - loss: 3.8036 - acc: 0.2432\n",
            "Epoch 81/100\n",
            "97393/97393 [==============================] - 36s 368us/step - loss: 3.7838 - acc: 0.2462\n",
            "Epoch 82/100\n",
            "97393/97393 [==============================] - 35s 364us/step - loss: 3.7742 - acc: 0.2471\n",
            "Epoch 83/100\n",
            "97393/97393 [==============================] - 35s 358us/step - loss: 3.7607 - acc: 0.2494\n",
            "Epoch 84/100\n",
            "97393/97393 [==============================] - 36s 366us/step - loss: 3.7473 - acc: 0.2511\n",
            "Epoch 85/100\n",
            "97393/97393 [==============================] - 35s 360us/step - loss: 3.7353 - acc: 0.2520\n",
            "Epoch 86/100\n",
            "97393/97393 [==============================] - 36s 369us/step - loss: 3.7198 - acc: 0.2556\n",
            "Epoch 87/100\n",
            "97393/97393 [==============================] - 36s 373us/step - loss: 3.7096 - acc: 0.2566\n",
            "Epoch 88/100\n",
            "97393/97393 [==============================] - 36s 368us/step - loss: 3.7012 - acc: 0.2576\n",
            "Epoch 89/100\n",
            "97393/97393 [==============================] - 36s 367us/step - loss: 3.6868 - acc: 0.2601\n",
            "Epoch 90/100\n",
            "97393/97393 [==============================] - 36s 365us/step - loss: 3.6765 - acc: 0.2623\n",
            "Epoch 91/100\n",
            "97393/97393 [==============================] - 35s 363us/step - loss: 3.6630 - acc: 0.2639\n",
            "Epoch 92/100\n",
            "97393/97393 [==============================] - 35s 359us/step - loss: 3.6519 - acc: 0.2647\n",
            "Epoch 93/100\n",
            "97393/97393 [==============================] - 36s 369us/step - loss: 3.6407 - acc: 0.2670\n",
            "Epoch 94/100\n",
            "97393/97393 [==============================] - 35s 357us/step - loss: 3.6299 - acc: 0.2679\n",
            "Epoch 95/100\n",
            "97393/97393 [==============================] - 37s 379us/step - loss: 3.6200 - acc: 0.2706\n",
            "Epoch 96/100\n",
            "97393/97393 [==============================] - 35s 363us/step - loss: 3.6048 - acc: 0.2722\n",
            "Epoch 97/100\n",
            "97393/97393 [==============================] - 36s 370us/step - loss: 3.5973 - acc: 0.2735\n",
            "Epoch 98/100\n",
            "97393/97393 [==============================] - 36s 368us/step - loss: 3.5893 - acc: 0.2741\n",
            "Epoch 99/100\n",
            "97393/97393 [==============================] - 36s 370us/step - loss: 3.5803 - acc: 0.2765\n",
            "Epoch 100/100\n",
            "97393/97393 [==============================] - 36s 368us/step - loss: 3.5695 - acc: 0.2789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0c6995b7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "D-JJkyXkwot1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def dante_says(text, output_len=25):\n",
        "  \n",
        "    output = \"\"\n",
        "  \n",
        "    for i in range(output_len):\n",
        "      tokens = np.array(tokenizer.texts_to_sequences([text]))\n",
        "      tokens = pad_sequences(tokens, maxlen=maxlen-1, truncating=\"pre\")\n",
        "      \n",
        "      pred_word = model.predict_classes([tokens])[0]\n",
        "      pred_word = tokenizer.index_word[pred_word]\n",
        "      \n",
        "      text+=\" \"+pred_word\n",
        "      output+=pred_word+\" \"\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNiEYysFqkTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "30202345-db07-41dc-ccce-be6dd6fbff08"
      },
      "cell_type": "code",
      "source": [
        "text = \"oggi è una bella giornata\"\n",
        "\n",
        "answer = dante_says(text)\n",
        "\n",
        "print(\"Giuseppe: \"+text)\n",
        "print(\"Dante: \"+answer)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Giuseppe: oggi è una bella giornata\n",
            "Dante: solaio e odono e domanda è fui ancor io che la vegg io mi disse che l mondo che l corpo dolorando stringonsi e quei \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YXoCrALyhHdx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PARCHEGGIO"
      ]
    }
  ]
}