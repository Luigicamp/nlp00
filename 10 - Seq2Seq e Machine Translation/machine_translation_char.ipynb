{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_translation_char.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/10%20-%20Seq2Seq%20e%20Machine%20Translation/machine_translation_char.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vgs3guLrm2jR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Architetture Sequence 2 Sequence per la traduzione automatica\n",
        "In questo notebook creeremo un'architettura **Sequence 2 Sequence** di rete neurale artificiale in grado di tradurre del testo dall'italiano all'inglese, per farlo avremo bisogno di un corpus di testo contenente frasi di esempio in entrambe le lingue, fortunatamente il software di flashcard Anki ci mette a disposizione tali corpus per molteplici lingue, scarichiamo quello per inglese-italiano da [questo link](http://www.manythings.org/anki/). Se usi Google colab o hai wget installato, scarica pure il file zip eseguendo la cella di codice qui sotto."
      ]
    },
    {
      "metadata": {
        "id": "UYis2kuN38JE",
        "colab_type": "code",
        "outputId": "450a1a32-266a-449b-a6bc-66a2129402a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.manythings.org/anki/ita-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-20 13:21:25--  http://www.manythings.org/anki/ita-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:30::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3981147 (3.8M) [application/zip]\n",
            "Saving to: ‘ita-eng.zip.1’\n",
            "\n",
            "ita-eng.zip.1       100%[===================>]   3.80M  1.90MB/s    in 2.0s    \n",
            "\n",
            "2019-04-20 13:21:27 (1.90 MB/s) - ‘ita-eng.zip.1’ saved [3981147/3981147]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "opMRzXz4nzVQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "ed estrai lo zip"
      ]
    },
    {
      "metadata": {
        "id": "AhiiXObg4Ai7",
        "colab_type": "code",
        "outputId": "cfd4ab84-8123-4970-d084-964f2a7e606b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip ita-eng.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ita-eng.zip\n",
            "replace ita.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ita.txt                 \n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4_kvjb7-n2cA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso abbiamo il file ita.txt contenente coppie di frasi in inglese e italiano su ogni riga, separate da un tab. Dividiamo le frasi in base alla lingua in due liste separate e salviamo ogni carattere all'interno di un set, mentre scorriamo il file processiamo anche le frasi, rimuovendo la punteggiatura e convertendo tutto in minuscolo."
      ]
    },
    {
      "metadata": {
        "id": "yanlWnxW4B1t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "en_chars = set({})\n",
        "it_chars = set({})\n",
        "\n",
        "it_sents = []\n",
        "en_sents = []\n",
        "\n",
        "with open(\"ita.txt\") as en_it_sents:\n",
        "  lines = en_it_sents.read().split(\"\\n\")\n",
        "  \n",
        "  for line in lines[:min(num_samples, len(lines)-1)]:\n",
        "    \n",
        "    line = re.sub(r'[^\\w\\s]','',line)\n",
        "    line = line.lower()\n",
        "    \n",
        "    en_sent, it_sent = line.split(\"\\t\")\n",
        "    en_sent = \"\\t\"+en_sent+\"\\n\"\n",
        "    \n",
        "    en_sents.append(en_sent)\n",
        "    it_sents.append(it_sent)\n",
        "    \n",
        "    for char in en_sent:\n",
        "      en_chars.add(char)\n",
        "      \n",
        "    for char in it_sent:\n",
        "      it_chars.add(char)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sX6Kzdnqp_MS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Convertiamo i set di caratteri in liste ordinate e contiamo il numer do caratteri contenuti in ogni lista."
      ]
    },
    {
      "metadata": {
        "id": "jq33OblVqBxy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "it_chars = sorted(list(it_chars))\n",
        "en_chars = sorted(list(en_chars))\n",
        "\n",
        "num_encoder_tokens = len(it_chars)\n",
        "num_decoder_tokens = len(en_chars)\n",
        "\n",
        "print('Numbero di token di input:', num_encoder_tokens)\n",
        "print('Number di token di output:', num_decoder_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SCWhad-3qI_r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Contiamo la lungezza massima di una frase per entrambe le lingue."
      ]
    },
    {
      "metadata": {
        "id": "oEld6kbH6ts5",
        "colab_type": "code",
        "outputId": "ef67ab17-708c-4ecb-f35f-a73e7d8a7e22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "cell_type": "code",
      "source": [
        "max_encoder_seq_length = max([len(sent) for sent in it_sents])\n",
        "max_decoder_seq_length = max([len(sent) for sent in en_sents])\n",
        "\n",
        "print(\"Lunghezza massima della sequenza per l'input:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 40\n",
            "Number of unique output tokens: 37\n",
            "Max sequence length for inputs: 39\n",
            "Max sequence length for outputs: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZoQNtRnGqsmu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso dobbiamo eseguire il one hot encoding di ogni frase a livello di carattere, per farlo ci conviene creare il dizionario che ci permette di accedere velocemente all'indice partendo dal carattere."
      ]
    },
    {
      "metadata": {
        "id": "dv89Vjd37s-s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "it_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(it_chars)])\n",
        "en_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(en_chars)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IinfEQC1q_cB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso eseguiamo il one hot encoding per creare:\n",
        "- **input dell'encodere**: le frasi italiane.\n",
        "- **input del decoder**: le frasi in inglese.\n",
        "- **output del decoder**: le frasi in inglese shiftate di un carattere."
      ]
    },
    {
      "metadata": {
        "id": "25Z6nmY67xkb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# inizializziamo gli array vuoti\n",
        "\n",
        "encoder_input_data = np.zeros(len(it_sents), max_encoder_seq_length, num_encoder_tokens)\n",
        "decoder_input_data = np.zeros((len(it_sents), max_decoder_seq_length, num_decoder_tokens))\n",
        "decoder_target_data = np.zeros((len(it_sents), max_decoder_seq_length, num_decoder_tokens))\n",
        "\n",
        "# iteriamo simultaneamente su frasi in italiano e inglese\n",
        "\n",
        "for i, (it_sent, en_sent) in enumerate(zip(it_sents, en_sents)):\n",
        "    for t, char in enumerate(it_sent):\n",
        "        # assegnamo un 1 all'indice di ogni carattere contenuto nella frase\n",
        "        encoder_input_data[i, t, it_token_index[char]] = 1.\n",
        "    for t, char in enumerate(en_sent):\n",
        "        # assegnamo un 1 all'indice di ogni carattere contenuto nella frase\n",
        "        decoder_input_data[i, t, en_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # shiftiamo di uno il target\n",
        "            decoder_target_data[i, t - 1, en_token_index[char]] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L97TUB63sbqk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso i nostri dati sono pronti, possiamo passare alla creazione del modello."
      ]
    },
    {
      "metadata": {
        "id": "BQQKb2WtpxwV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creazione del Modello\n",
        "Il modello che andremo a creare è molto complesso e alcuni dei suoi strati devono accettare input da strati differenti (il decoder prende lo stato dall'encoder e come input le frasi in inglese), dobbiamo utilizzare le [API Funzionali di Keras](https://keras.io/getting-started/functional-api-guide/). \n",
        "<br>\n",
        "Cominciamo con l'**encoder**:\n",
        "- Usiamo la classe **Input** per definire l'input dell'encoder.\n",
        "- Creiamo lo strato ricorrente (LSTM) dell'encoder, che dovrà restituire lo stato (return_state=True)\n",
        "- Usiamo l'encoder, scartiamo l'ouput (primo array ritornato) e teniamo solo lo stato (secondo e terzo array)\n"
      ]
    },
    {
      "metadata": {
        "id": "wsvK1SOL73xh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, LSTM, Dense\n",
        "\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(256, return_state=True)\n",
        "_, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QsFyK7iOwC0e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso creiamo il **decoder**:\n",
        "- Usiamo la classe **Input** per definire l'input dell decoder.\n",
        "- Creiamo lo strato ricorrente (LSTM) dell'encoder, che dovrà usare lo stato dell'encoder (initial_state=encoder_states)\n",
        "- Usiamo l'encoder, scartiamo l'ouput (primo array ritornato) e teniamo solo lo stato (secondo e terzo array).\n",
        "- Creiamo uno strato di output che utilizzerà la funzine di attivazione softmax per eseguire una classificazione multiclasse.\n"
      ]
    },
    {
      "metadata": {
        "id": "hvzQXCZOs1r0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fRqMX6ZHx-RR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Adesso usiamo la classe **Model** di keras per creare il modello, passando gli strati di input (sia quello dell'encoder che quello del decoder) all'interno di una lista e lo strato di output."
      ]
    },
    {
      "metadata": {
        "id": "4XVB7mpCtOKV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F-5yPt0syNqD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compiliamo il modello, usiamo come algoritmo di ottimizzazione 'rmsprop', il quale dovrebbe portare risultati migliori quando si lavora con modelli sequenziali."
      ]
    },
    {
      "metadata": {
        "id": "o25hndZK8VpL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q4w7G7slyXeH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ed avviamo l'addestramento, qui dobbiamo passare sempre i dati di input, sia per l'encoder che per il decoder, all'interno di una lista e i dati di output."
      ]
    },
    {
      "metadata": {
        "id": "HwmryR4V8dIg",
        "colab_type": "code",
        "outputId": "5efec382-142c-4871-8b06-6738290d8eb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3500
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=256,\n",
        "          epochs=100,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 11s 1ms/step - loss: 2.0902 - acc: 0.1964 - val_loss: 2.2545 - val_acc: 0.2288\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 1.7717 - acc: 0.2689 - val_loss: 2.0283 - val_acc: 0.2942\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 1.5651 - acc: 0.3150 - val_loss: 1.8585 - val_acc: 0.3499\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 1.4421 - acc: 0.3442 - val_loss: 1.8498 - val_acc: 0.3484\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 1.3509 - acc: 0.3695 - val_loss: 1.7392 - val_acc: 0.3674\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 1.2755 - acc: 0.3892 - val_loss: 1.7097 - val_acc: 0.3993\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 1.2089 - acc: 0.4097 - val_loss: 1.6694 - val_acc: 0.4104\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 1.1583 - acc: 0.4250 - val_loss: 1.6788 - val_acc: 0.4064\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 1.1186 - acc: 0.4396 - val_loss: 1.5267 - val_acc: 0.4556\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 1.0622 - acc: 0.4549 - val_loss: 1.6808 - val_acc: 0.3986\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 1.0030 - acc: 0.4730 - val_loss: 1.5937 - val_acc: 0.4533\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.9536 - acc: 0.4863 - val_loss: 1.6295 - val_acc: 0.4466\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.9059 - acc: 0.4997 - val_loss: 1.6101 - val_acc: 0.4635\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.8614 - acc: 0.5157 - val_loss: 1.5846 - val_acc: 0.4648\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.8152 - acc: 0.5299 - val_loss: 1.5377 - val_acc: 0.4816\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.7814 - acc: 0.5401 - val_loss: 1.5390 - val_acc: 0.4742\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.7478 - acc: 0.5501 - val_loss: 1.5462 - val_acc: 0.4810\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.7174 - acc: 0.5606 - val_loss: 1.6026 - val_acc: 0.4745\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.6899 - acc: 0.5680 - val_loss: 1.7375 - val_acc: 0.4677\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.6638 - acc: 0.5764 - val_loss: 1.6076 - val_acc: 0.4815\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.6403 - acc: 0.5828 - val_loss: 1.6561 - val_acc: 0.4840\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.6182 - acc: 0.5896 - val_loss: 1.6652 - val_acc: 0.4811\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5962 - acc: 0.5966 - val_loss: 1.6947 - val_acc: 0.4862\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5775 - acc: 0.6019 - val_loss: 1.7551 - val_acc: 0.4757\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5592 - acc: 0.6065 - val_loss: 1.6801 - val_acc: 0.4879\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5419 - acc: 0.6104 - val_loss: 1.7957 - val_acc: 0.4767\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.5266 - acc: 0.6148 - val_loss: 1.7825 - val_acc: 0.4853\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.5114 - acc: 0.6187 - val_loss: 1.7401 - val_acc: 0.4907\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4976 - acc: 0.6220 - val_loss: 1.8080 - val_acc: 0.4770\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4843 - acc: 0.6263 - val_loss: 1.7885 - val_acc: 0.4911\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4720 - acc: 0.6298 - val_loss: 1.7159 - val_acc: 0.5004\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4602 - acc: 0.6326 - val_loss: 1.7684 - val_acc: 0.4950\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4503 - acc: 0.6355 - val_loss: 1.8087 - val_acc: 0.4919\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4395 - acc: 0.6382 - val_loss: 1.8903 - val_acc: 0.4896\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4293 - acc: 0.6410 - val_loss: 1.8907 - val_acc: 0.4960\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4204 - acc: 0.6444 - val_loss: 1.8649 - val_acc: 0.4960\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.4122 - acc: 0.6456 - val_loss: 1.8117 - val_acc: 0.5016\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.4042 - acc: 0.6480 - val_loss: 1.8304 - val_acc: 0.4986\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3948 - acc: 0.6510 - val_loss: 1.9121 - val_acc: 0.4968\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3872 - acc: 0.6521 - val_loss: 1.9205 - val_acc: 0.5000\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3794 - acc: 0.6548 - val_loss: 1.9175 - val_acc: 0.4957\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3723 - acc: 0.6570 - val_loss: 1.9066 - val_acc: 0.5009\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3644 - acc: 0.6594 - val_loss: 1.8981 - val_acc: 0.4985\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3570 - acc: 0.6618 - val_loss: 1.9741 - val_acc: 0.4982\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3513 - acc: 0.6631 - val_loss: 1.9673 - val_acc: 0.4991\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3445 - acc: 0.6644 - val_loss: 1.9667 - val_acc: 0.5028\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3376 - acc: 0.6666 - val_loss: 2.0214 - val_acc: 0.4982\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3314 - acc: 0.6692 - val_loss: 2.0227 - val_acc: 0.5001\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3250 - acc: 0.6707 - val_loss: 2.0672 - val_acc: 0.4950\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.3174 - acc: 0.6733 - val_loss: 1.9731 - val_acc: 0.5025\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3124 - acc: 0.6749 - val_loss: 2.0103 - val_acc: 0.4987\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3068 - acc: 0.6764 - val_loss: 2.0726 - val_acc: 0.5012\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.3004 - acc: 0.6781 - val_loss: 2.0524 - val_acc: 0.5060\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2950 - acc: 0.6805 - val_loss: 2.0919 - val_acc: 0.5005\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2877 - acc: 0.6816 - val_loss: 2.1499 - val_acc: 0.4999\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2824 - acc: 0.6847 - val_loss: 2.1165 - val_acc: 0.5043\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2759 - acc: 0.6860 - val_loss: 2.1435 - val_acc: 0.5018\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2700 - acc: 0.6886 - val_loss: 2.1155 - val_acc: 0.5020\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2650 - acc: 0.6898 - val_loss: 2.1419 - val_acc: 0.5058\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2577 - acc: 0.6922 - val_loss: 2.1976 - val_acc: 0.4981\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2524 - acc: 0.6936 - val_loss: 2.2285 - val_acc: 0.5023\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2463 - acc: 0.6963 - val_loss: 2.2141 - val_acc: 0.4993\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.2402 - acc: 0.6975 - val_loss: 2.2241 - val_acc: 0.5102\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2351 - acc: 0.6991 - val_loss: 2.2394 - val_acc: 0.5027\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2283 - acc: 0.7014 - val_loss: 2.2265 - val_acc: 0.4999\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2234 - acc: 0.7037 - val_loss: 2.2798 - val_acc: 0.5021\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2177 - acc: 0.7054 - val_loss: 2.3151 - val_acc: 0.5002\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2117 - acc: 0.7076 - val_loss: 2.3101 - val_acc: 0.4991\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2057 - acc: 0.7096 - val_loss: 2.3388 - val_acc: 0.4963\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.2017 - acc: 0.7107 - val_loss: 2.3407 - val_acc: 0.4980\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1951 - acc: 0.7131 - val_loss: 2.3368 - val_acc: 0.5012\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1914 - acc: 0.7141 - val_loss: 2.3734 - val_acc: 0.5012\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1846 - acc: 0.7161 - val_loss: 2.3491 - val_acc: 0.4966\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1795 - acc: 0.7184 - val_loss: 2.4707 - val_acc: 0.4931\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1743 - acc: 0.7199 - val_loss: 2.3920 - val_acc: 0.5058\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1698 - acc: 0.7215 - val_loss: 2.3786 - val_acc: 0.5036\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1641 - acc: 0.7238 - val_loss: 2.4112 - val_acc: 0.5035\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1599 - acc: 0.7246 - val_loss: 2.4209 - val_acc: 0.5023\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1541 - acc: 0.7265 - val_loss: 2.4750 - val_acc: 0.5031\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1491 - acc: 0.7285 - val_loss: 2.4748 - val_acc: 0.5018\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1451 - acc: 0.7297 - val_loss: 2.5505 - val_acc: 0.4970\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1395 - acc: 0.7317 - val_loss: 2.4999 - val_acc: 0.5026\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1350 - acc: 0.7332 - val_loss: 2.5345 - val_acc: 0.5020\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1321 - acc: 0.7337 - val_loss: 2.5684 - val_acc: 0.5001\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1273 - acc: 0.7357 - val_loss: 2.5316 - val_acc: 0.5016\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1233 - acc: 0.7375 - val_loss: 2.5540 - val_acc: 0.5013\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1194 - acc: 0.7380 - val_loss: 2.5809 - val_acc: 0.5024\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1158 - acc: 0.7393 - val_loss: 2.6004 - val_acc: 0.4968\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.1113 - acc: 0.7411 - val_loss: 2.6025 - val_acc: 0.5005\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1094 - acc: 0.7414 - val_loss: 2.6244 - val_acc: 0.4988\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1046 - acc: 0.7436 - val_loss: 2.7886 - val_acc: 0.4871\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.1028 - acc: 0.7436 - val_loss: 2.6427 - val_acc: 0.4996\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0999 - acc: 0.7440 - val_loss: 2.6419 - val_acc: 0.5007\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0965 - acc: 0.7453 - val_loss: 2.7064 - val_acc: 0.4998\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0940 - acc: 0.7469 - val_loss: 2.7599 - val_acc: 0.4943\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0912 - acc: 0.7471 - val_loss: 2.7462 - val_acc: 0.4976\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 10s 1ms/step - loss: 0.0880 - acc: 0.7489 - val_loss: 2.8078 - val_acc: 0.4891\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0864 - acc: 0.7489 - val_loss: 2.7334 - val_acc: 0.4986\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0838 - acc: 0.7497 - val_loss: 2.7619 - val_acc: 0.4966\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 9s 1ms/step - loss: 0.0816 - acc: 0.7506 - val_loss: 2.8026 - val_acc: 0.4942\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f438b0923c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "WziMjgjcp2mb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Usare il modello per tradurre del testo"
      ]
    },
    {
      "metadata": {
        "id": "kSzBCb1F8fmM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sPULvJtX9B7-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reverse_it_char_index = dict(\n",
        "    (i, char) for char, i in it_token_index.items())\n",
        "reverse_en_char_index = dict(\n",
        "    (i, char) for char, i in en_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0M1nsXAi9Nky",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, en_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_en_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8_ONvW4-aJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "  text_encoded = np.zeros((max_encoder_seq_length, num_encoder_tokens))\n",
        "\n",
        "  for c, char in enumerate(text):\n",
        "        text_encoded[c, it_token_index[char]] = 1.\n",
        "      \n",
        "  return np.array([text_encoded])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PlIyinUD-Ch6",
        "colab_type": "code",
        "outputId": "525a927b-40d3-43d1-f47f-bd919d129225",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "text = \"io ho fame\"\n",
        "input_seq = encode(text)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "\n",
        "print('Italiano:', text)\n",
        "print('English:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Italiano: io ho fame\n",
            "English: i am hungry\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}