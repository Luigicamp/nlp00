{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfAI/nlp00/blob/master/4%20-%20Preprocessing%20del%20testo/text_preprocessing_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "lIngrrjZR0C-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tokenizzazione\n",
        "La tokenizzazione è il processo di estrazione delle parti costituenti del testo, chiamati per l'appunto **tokens**, in sostanza i tokens sono le parole e i caratteri di punteggiatura che compongono il testo.\n",
        "<br>\n",
        "Vediamo come eseguire la tokenizzazione con nltk, importiamo il modulo"
      ]
    },
    {
      "metadata": {
        "id": "CKp4HdRwP9Il",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mG2uOccZsIg6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Per eseguire la tokenizzazione, dobbiamo scaricare il pacchetto di dati 'punkt'."
      ]
    },
    {
      "metadata": {
        "id": "trbKIsjMRbXy",
        "colab_type": "code",
        "outputId": "98fe6c36-22a9-4af1-e542-553b51ae847a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "PUZubpMOspJv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Per eseguire la tokenizzazione possiamo usare la funzione *word_tokenizer(sentence)* di nltk."
      ]
    },
    {
      "metadata": {
        "id": "qbkzAN_4Rkpk",
        "colab_type": "code",
        "outputId": "28b505ed-7c21-43b4-c4e4-ba56c13e31a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "string = \"Cos'è questa fretta? Facciamolo un'altra volta, ti va bene?\"\n",
        "\n",
        "tokens = word_tokenize(string)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Cos'è\", 'questa', 'fretta', '?', 'Facciamolo', \"un'altra\", 'volta', ',', 'ti', 'va', 'bene', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ns0A9_sPtf4g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi il risultato è una lista dei tokens, cioè di parole e punteggiatura che costituiscono la frase, ma non avremmo ottenuto lo stesso risultato usando il metodo *split()* ? Vediamolo"
      ]
    },
    {
      "metadata": {
        "id": "Dp_MmD2Ttzrc",
        "colab_type": "code",
        "outputId": "37a2443f-d397-4f57-afb1-4e7c8c3cf81c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "cell_type": "code",
      "source": [
        "string = \"Cos'è questa fretta? Facciamolo un'altra volta, ti va bene?\"\n",
        "string.split()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Cos'è\",\n",
              " 'questa',\n",
              " 'fretta?',\n",
              " 'Facciamolo',\n",
              " \"un'altra\",\n",
              " 'volta,',\n",
              " 'ti',\n",
              " 'va',\n",
              " 'bene?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "nrs0h3YGt7Ko",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi il risultato non è lo stesso, il metodo *split()* semplicemente separa le parole in base agli spazi, quindi la punteggiatura non viene isolata.\n",
        "<br>\n",
        "Oltre a questo la tokenizzazione ci dovrebbe permette di dividere parole unite in forma contratta (Cos'è = Cosa+è). Purtroppo la funzione *word_tokenizer* non supporta la lingua italiana, facciamo una prova con l'inglese."
      ]
    },
    {
      "metadata": {
        "id": "mUeC0RIpQAA7",
        "colab_type": "code",
        "outputId": "f3a69bd1-8f23-4e25-afc5-f515f91b33be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "string = \"I'd love to visit U.S.A, but I can't afford it!\" # Mi piacerebbe visitare gli Stati Uniti ma non posso permettermelo\n",
        "\n",
        "tokens = nltk.word_tokenize(string)\n",
        "print(tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', \"'d\", 'love', 'to', 'visit', 'U.S.A', ',', 'but', 'I', 'ca', \"n't\", 'afford', 'it', '!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AoWF_gBJv5FA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi, con la lingua inglese, la funzione *word_tokenizer* è riuscita a separare anche le parole unite in forma contratta, nota anche come è riuscita a comprendere che U.S.A è un'unica parola."
      ]
    },
    {
      "metadata": {
        "id": "ava0Sk6yZb6D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stop Words\n",
        "Le stop words sono quelle parole comuni del linguaggio che portano poca o nessuna informazione al discorso, esempi possono essere le congiunzioni o verbi comuni come il verbo essere e il verbo avere.\n",
        "<br>\n",
        "NLTK contiene liste di stop words per diversi linguaggi, per ottenerle dobbiamo prima scaricare il corpus 'stopwords'."
      ]
    },
    {
      "metadata": {
        "id": "MZtHEWhcaEV2",
        "colab_type": "code",
        "outputId": "ffc6778f-9710-4825-cbbc-a5130944bc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "YPyGJZn1arEG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Poi possiamo ottenere una lista per una determinata lingua utilizzando la funzione *words* del metodo *stopwords*, indicando al suo interno la lingua per la quale vogliamo avere le stop words."
      ]
    },
    {
      "metadata": {
        "id": "1XdTxFy3Z5lx",
        "colab_type": "code",
        "outputId": "4534359d-bba8-42b8-a9fc-324d0a5b185a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "en_stopwords = stopwords.words('english')\n",
        "\n",
        "print(\"Stop words totali: %d\" % len(en_stopwords))\n",
        "print(\"Prime 10 stop words: %s\" % en_stopwords[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stop words totali: 179\n",
            "Prime 10 stop words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "phitv0LTa8Ih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi per l'inglese abbiamo 179 stop words, vediamo per la lingua italiana."
      ]
    },
    {
      "metadata": {
        "id": "EjNIQLLFafK_",
        "colab_type": "code",
        "outputId": "481b414e-59a9-4fc1-c8cc-398c45fe9826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "it_stopwords = stopwords.words('italian')\n",
        "\n",
        "print(\"Stop words totali: %d\" % len(it_stopwords))\n",
        "print(\"Prime 10 stop words: %s\" % it_stopwords[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stop words totali: 279\n",
            "Prime 10 stop words: ['ad', 'al', 'allo', 'ai', 'agli', 'all', 'agl', 'alla', 'alle', 'con']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LP7X7neAbEL4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Per l'italiano abbiamo 279 parole, possiamo rimuovere le stop words dalla nostra lista di tokens iterando sopra di essi e verificando se il token è presente o meno nella lista di stop words."
      ]
    },
    {
      "metadata": {
        "id": "WHHXpHzWboJe",
        "colab_type": "code",
        "outputId": "c4eb00ec-4f73-4f72-f393-0f99063d2a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "string = \"Io sono una persona simpatica, solo che non mi piacciono gli esseri umani, preferisco i gatti\"\n",
        "\n",
        "it_stopwords = stopwords.words('italian')\n",
        "\n",
        "tokens = word_tokenize(string)\n",
        "\n",
        "tokens_filtered = []\n",
        "\n",
        "for token in tokens:\n",
        "  if(token.lower() not in it_stopwords):\n",
        "    tokens_filtered.append(token)\n",
        "    \n",
        "    \n",
        "print(\"Tokens originali: %s\" % tokens)\n",
        "print(\"Tokens rimasti: %s\" % tokens_filtered)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens originali: ['Io', 'sono', 'una', 'persona', 'simpatica', ',', 'solo', 'che', 'non', 'mi', 'piacciono', 'gli', 'esseri', 'umani', ',', 'preferisco', 'i', 'gatti']\n",
            "Tokens rimasti: ['persona', 'simpatica', ',', 'solo', 'piacciono', 'esseri', 'umani', ',', 'preferisco', 'gatti']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hONBoj60cldY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi abbiamo convertito la parola in minuscolo per assicurarsi il matching anche delle parole a inizio frase."
      ]
    },
    {
      "metadata": {
        "id": "xiOr2eubR3MF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "Lo stemming è un processo che consiste nel ridurre le parole di un testo nella loro forma base, chiamata appunto **stem**.\n",
        "<br><br>\n",
        "NLTK ci mette a disposizione due oggetti per eseguire lo stemming, il PorterStemmer e lo SnowballStemmer."
      ]
    },
    {
      "metadata": {
        "id": "JzMDMKHC4L5q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Porter Stemmer\n",
        "Il Poter Stemmer è un'algoritmo di stemming che applica una serie di regole in 5 steps per rimuovere il suffisso dalla frase.\n",
        "<br>\n",
        "Importiamo la classe *PorterStemmer* da nltk e testiamola su una frase, questo algoritmo funziona solo con la lingua inglese."
      ]
    },
    {
      "metadata": {
        "id": "LVEeEfNUQFzp",
        "colab_type": "code",
        "outputId": "540bd16e-8576-427e-9d5a-c1753cdd87d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "string = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
        "tokens = nltk.word_tokenize(string)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# lo stemmer prende in input la singola parola\n",
        "# quindi passiamo un token per volta\n",
        "# utilizzando un ciclo for\n",
        "\n",
        "print(\"TOKEN\\t\\tSTEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\t\tI\n",
            "fed\t\tfed\n",
            "my\t\tmy\n",
            "cats\t\tcat\n",
            "with\t\twith\n",
            "some\t\tsome\n",
            "mice\t\tmice\n",
            "while\t\twhile\n",
            "playing\t\tplay\n",
            "Candy\t\tcandi\n",
            "Crush\t\tcrush\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rc2Yw_kf7VYq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Snowball Stemmer\n",
        "Snowball è un'altro algoritmo di stemming che apporta alcune migliorie al Porter Stemmer e a differenza di questo supporta diverse lingue con NLTK, incluso l'italiano.\n",
        "<br>\n",
        "Possiamo passare la lingua come parametro dello stemmer."
      ]
    },
    {
      "metadata": {
        "id": "2VcSKtRN7kY4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JHd6iP4ESa5w",
        "colab_type": "code",
        "outputId": "4df705cc-9355-4920-d541-c950b75eee7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "string = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
        "tokens = nltk.word_tokenize(string)\n",
        "\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "print(\"TOKEN\\t\\tSTEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN\t\tSTEM\n",
            "I\t\ti\n",
            "fed\t\tfed\n",
            "my\t\tmy\n",
            "cats\t\tcat\n",
            "with\t\twith\n",
            "some\t\tsome\n",
            "mice\t\tmice\n",
            "while\t\twhile\n",
            "playing\t\tplay\n",
            "King\t\tking\n",
            "of\t\tof\n",
            "the\t\tthe\n",
            "Riddles\t\triddl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8MeHyejFTuea",
        "colab_type": "code",
        "outputId": "a9a34c65-11bb-4b87-da03-91eb581d9a01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "string = \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\"\n",
        "tokens = nltk.word_tokenize(string)\n",
        "\n",
        "stemmer = SnowballStemmer(\"italian\")\n",
        "\n",
        "print(\"TOKEN\\t\\tSTEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ho',\n",
              " 'sfam',\n",
              " 'i',\n",
              " 'mie',\n",
              " 'gatt',\n",
              " 'con',\n",
              " 'dei',\n",
              " 'top',\n",
              " 'mentr',\n",
              " 'gioc',\n",
              " 'a',\n",
              " 'il',\n",
              " 'signor',\n",
              " 'degl',\n",
              " 'enigm']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "En69Ke2j8Djh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ovviamente la precisione dell'algoritmo è maggiore per la lingua inglese che per la lingua italiana."
      ]
    },
    {
      "metadata": {
        "id": "C5PFDdUHPzQk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lancaster Stemmer\n",
        "Il Lancaster è un'altro algoritmo di stemming che applica regole più aggressive per la riduzione delle parole, anche eccessivamente aggressive, facciamo un esempio."
      ]
    },
    {
      "metadata": {
        "id": "VWA20mfXP1ie",
        "colab_type": "code",
        "outputId": "a7c84e2f-0d15-412b-8b6a-2383cb833612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "\n",
        "string = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
        "tokens = nltk.word_tokenize(string)\n",
        "\n",
        "stemmer=LancasterStemmer()\n",
        "\n",
        "print(\"TOKEN\\t\\tSTEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token, stemmer.stem(token)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN\t\tSTEM\n",
            "My\t\tmy\n",
            "cats\t\tcat\n",
            "ate\t\tat\n",
            "some\t\tsom\n",
            "mice\t\tmic\n",
            "while\t\twhil\n",
            "I\t\ti\n",
            "was\t\twas\n",
            "playing\t\tplay\n",
            "King\t\tking\n",
            "of\t\tof\n",
            "the\t\tthe\n",
            "Riddles\t\triddl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uyOr_FaxQaVw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi usando il Lancaster alcune parole sono state tagliate. Nella pratica possiamo adottare tranquillamente lo Snowball, il quale dovrebbe portare a risultati più stabili e precisi."
      ]
    },
    {
      "metadata": {
        "id": "SUrgr8xbErxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lemmatizatione\n",
        "La lemmatizzazione ci permette di ridurre una parola dalla sua forma flessa alla sua forma canonica, detta appunto **lemma**, il quale a differenza dello stem è una parola reale.\n",
        "Possiamo eseguire la lemmatizzazione con nltk usando il *WordNetLemmatizer*."
      ]
    },
    {
      "metadata": {
        "id": "iZ_qaI2yMp37",
        "colab_type": "code",
        "outputId": "f4f1a463-2cc9-4de3-b2c8-ff700a8fd46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "u8siCFEnEqmB",
        "colab_type": "code",
        "outputId": "22caf080-b76e-443c-c4b5-f062e08f63d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "string = \"My cats ate some mice while I was playing King of the Riddles\" # \"I miei gatti hanno mangiato alcuni topi mentre io stavo giocavo a il signore degli enigmi\n",
        "tokens = nltk.word_tokenize(string)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"TOKEN\\t\\tLEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token, lemmatizer.lemmatize(token)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN\t\tLEM\n",
            "My\t\tMy\n",
            "cats\t\tcat\n",
            "ate\t\tate\n",
            "some\t\tsome\n",
            "mice\t\tmouse\n",
            "while\t\twhile\n",
            "I\t\tI\n",
            "was\t\twa\n",
            "playing\t\tplaying\n",
            "King\t\tKing\n",
            "of\t\tof\n",
            "the\t\tthe\n",
            "Riddles\t\tRiddles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UFXy2CVOXD82",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Come vedi a lemmatizzato perfettamente i nomi (mice -> mouse), però non ha fatto altrettanto con i verbi, perchè ? Perché dobbiamo anche specificare il tipo di parola utilizzando il parametro pos, se non lo facciamo crederà che si tratti di un nome.\n"
      ]
    },
    {
      "metadata": {
        "id": "i3wH36NLMfhB",
        "colab_type": "code",
        "outputId": "220dfa2a-3d7f-4eec-e186-17391408694f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokens = [(\"My\",\"n\"),(\"cats\",\"n\"),(\"ate\",\"v\"), (\"some\",\"n\"), (\"mice\",\"n\"), (\"while\",\"r\"), (\"I\",\"n\"), (\"was\",\"v\"), (\"playing\",\"v\"), (\"King\",\"v\"), (\"of\",\"n\"),(\"the\",\"n\"),(\"Riddles\",\"n\")]\n",
        "\n",
        "print(\"TOKEN\\t\\tLEM\")\n",
        "\n",
        "for token in tokens:\n",
        "  print(\"%s\\t\\t%s\" % (token[0], lemmatizer.lemmatize(token[0], pos=token[1])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TOKEN\t\tLEM\n",
            "My\t\tMy\n",
            "cats\t\tcat\n",
            "ate\t\teat\n",
            "some\t\tsome\n",
            "mice\t\tmouse\n",
            "while\t\twhile\n",
            "I\t\tI\n",
            "was\t\tbe\n",
            "playing\t\tplay\n",
            "King\t\tKing\n",
            "of\t\tof\n",
            "the\t\tthe\n",
            "Riddles\t\tRiddles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1IaEPSmCY0n6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pos sta per part-of-speech, cioè parte del discorso, è possibile automatizzare l'identificazione della parte del discorso in modo da non doverli inserire manualmente, nella prossima sezione vedremo come."
      ]
    }
  ]
}